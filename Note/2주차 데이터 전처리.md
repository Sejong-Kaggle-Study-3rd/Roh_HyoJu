# 데이터 전처리

복습: No
완료여부: 완료
유형: 스터디 그룹
자료: https://github.com/sejongresearch/2020.MachineLearning/blob/master/LectureNote/2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20(%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8).pdf
주차: 2주차

## 데이터 전처리

( 데이터 품질은 데이터 분석의 90% 좌우)

→ **데이터의 품질을 올리는 과정**

- 과정

    데이터 실수화

    불완전 데이터 제거 → NULL, NA, NAN 제거

    잡음 섞인 데이터 제거

    모순된 데이터 제거

    불균형 데이터 해결 ( 과소표집, 과대표집) 

- 주요 기법
    1. 데이터 실수화 (Data Vectorization)
    2. 데이터 정제 (Data Cleaning) → 불완전 데이터, 잡음 데이터, 모순데이터 제거
    3. 데이터 통합 (Data Integration)
    4. 데이터 축소 (Data Reduction) → 데이터 수 줄임(Sampling) / 데이터 차원 축소
    5. 데이터 변환 (Data Transformation) → 정규화
    6. 데이터 균형 (Data Balancing) → sampling으로 해결

### 데이터 실수화

→ 실수로 구성된 형태로 전환 ([n_sample, n_features]→ 행렬/2차원 텐서)

- 자료의 유형
    1. 연속형 자료
        - 구간을 나눠서 구간마다 index or lable 지정
    2. 범주형 자료
        - One-hot encoding으로 실수화

            Ex) Scikit-learn의 DictVectorizer 함수

        ```python
        from sklearn.feature_extraction import DicVectorizer
        vec=DictVectorizer(sparse=False)
        vec.fit_transform(x) #x는 수량화하고 싶은 범주형 자료
        ```

        - 희소행렬(Sparse Matrix)

            → 행렬의 값이 대부분 0인 경우 → 메모리 낭비 심함 → COO 표현식과 CSR 표현식 통해 해결

            - CSR 표현식 (sparse=True default 값)

                → 메모리 적게 / 빠른 연산

                → 희소행렬을 눈에 볼 수 X

                ```python
                vec=DictVectorizer(sparse=True)
                ```

    3. 텍스트 자료
        - 단어의 출현 횟수를 이용한 데이터 실수화
            - 출현 횟수가 정보의 양과 비례 X → TF-IDF 기법 사용

                → 자주 등장해서 분석에 의미없는 단어 중요도 낮추는 기법(ex. The, a ...)

        ```python
        from sklearn.feature_extraction.text import CountVectorizer
        vec=CountVectorizer()
        t=vec.fit_transform(text)
        t.toarray() # toarray():CSR표현의 압축을 풀기 위해 사용
        ```

        → 단순히 출현 횟수로만 실수화

        ![%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%205fb9bf1f7a384163b6221f2682434dea/Untitled.png](%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%205fb9bf1f7a384163b6221f2682434dea/Untitled.png)

        - TF-IDF

            → 가중치 재계산

            → 높은 빈도에 낮은 가중치, 낮은 빈도에 높은 가중치

            ```python
            from sklearn.feature_extraction.text import TfidfVectorizer
            tfid=TfidfVectorizer()
            x=tfid.fit_transform(text).toarray()
            ```

            → 빈도에 따른 가중치로 수정되어 실수화됨

            ![%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%205fb9bf1f7a384163b6221f2682434dea/Untitled%201.png](%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%205fb9bf1f7a384163b6221f2682434dea/Untitled%201.png)

### 데이터 변환

- 필요성

    → 데이터가 가진 특성 비교하여 패턴을 찾음

    → 특성 간 스케일 차이가 심하면 패턴 찾는데 어려움

- 변환법
    - 표준화(Standardization)

        $$(x-mean(x))/sd(x)$$

    - 정규화(Normalization)

    $$(x-min(x))/(max(x)-min(x))$$

    - 일반적으로 정규화가 표준화보다 유용
    - Bell-shape이거나 이상치 존재 → 표준화 유용

    ![%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%205fb9bf1f7a384163b6221f2682434dea/Untitled%202.png](%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%205fb9bf1f7a384163b6221f2682434dea/Untitled%202.png)

### 데이터 정제

- 결측치 채우기
    - None, np.nan, npNaN
    - 평균, 중위수, 최빈수로 대처

    Ex) sklearn의 SimpleImputer(): 입력인자로 평균(mean), 중위수(median), 최빈수(most_frequent) 선택

    ```python
    from sklearn.impute import SimpleImputer
    im=SimpleImputer(missing_values=np.nan, strategy='mean')
    im.fit(x) #열의 평균값으로 대체
    ```

### 데이터 통합

→ 여러 개의 데이터 파일을 하나로 합치는 과정

Ex) Pandas의 merge()

```python
import pandas as pd
df1=pd.read_csv("데이터")
df2=pd.read_csv("데이터2")
df=pd.merge(df1,df2)
df.shape #행/열개수 반환(행,열)
df.dtypes #변수의 자료 타입 확인
```

### 데이터 불균형

→ 목적이 분류일 때 특정 클래스의 관측치가 다른 클래스에 비해 매우 낮게 나타나는 것

- 해소기법
    - 과소표집(undersampling) or 과대표집(oversampling)
    - 일반적으로 과대표집이 통계적으로 유용
    - Decision tree나 ensemble → 불균형자료에 강인

- 과소표집

    → 다수 클래스의 표본을 임의로 학습데이터로부터 제거

- 과대표집

    → 소수 클래스의 표본을 복제하여 학습데이터에 추가

    Ex) SMOTE, ADASYN

```python
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE, ADASYN

x,y=make_classification(n_classes=3, n_feature=2, n_sample=200, n_informative=2, n_redundant=0, random_state=10)
# class 3개, feature 2개(2차원 데이터), sample 개수 200개

import matplotlib.pyplot as plt
plt.scatter() #스캐터플롯

sm=SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(x,y)

ada=ADASYN(random_state=0)
X_syn, y_syn=ada.fit_resample(x,y)

from imblearn.under_sampling import NearMiss
undersample=NearMiss(version=3, n_neighbors_ver3=3)
X_under, y_under = undersample.fit_resample(x,y)
```

- 여러가지 그래프 그리기(참고)

[데이터 사이언스 스쿨](https://datascienceschool.net/01%20python/05.05%20%ED%8C%90%EB%8B%A4%EC%8A%A4%EC%9D%98%20%EC%8B%9C%EA%B0%81%ED%99%94%20%EA%B8%B0%EB%8A%A5.html)